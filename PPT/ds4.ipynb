{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "General Linear Model:\n",
    "\n",
    "1. What is the purpose of the General Linear Model (GLM)?\n",
    "2. What are the key assumptions of the General Linear Model?\n",
    "3. How do you interpret the coefficients in a GLM?\n",
    "4. What is the difference between a univariate and multivariate GLM?\n",
    "5. Explain the concept of interaction effects in a GLM.\n",
    "6. How do you handle categorical predictors in a GLM?\n",
    "7. What is the purpose of the design matrix in a GLM?\n",
    "8. How do you test the significance of predictors in a GLM?\n",
    "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
    "10. Explain the concept of deviance in a GLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Linear Model:\n",
    "1. The purpose of the General Linear Model (GLM) is to analyze the relationship between a dependent variable and independent variables by modeling their linear or nonlinear associations, categorical effects, and interactions.\n",
    "2. The key assumptions of the GLM are linearity (relationship between variables), independence of observations, homoscedasticity (constant variance of residuals), and normality of residuals.\n",
    "3. Coefficients in a GLM represent the change in the mean value of the dependent variable for a unit change in the corresponding independent variable, assuming all other variables are held constant.\n",
    "4. A univariate GLM analyzes the relationship between a single dependent variable and independent variables, while a multivariate GLM simultaneously analyzes multiple dependent variables and their relationship with independent variables.\n",
    "5. Interaction effects in a GLM occur when the effect of one independent variable on the dependent variable varies based on the levels of another independent variable.\n",
    "6. Categorical predictors in a GLM can be handled using dummy variables or indicator variables.\n",
    "7. The design matrix in a GLM represents the relationship between the dependent variable and independent variables, enabling the estimation of coefficients and statistical analysis.\n",
    "8. The significance of predictors in a GLM is tested using hypothesis testing, typically through t-tests for individual coefficients.\n",
    "9. Type I, Type II, and Type III sums of squares are different methods for partitioning the variability in the dependent variable among independent variables in a GLM.\n",
    "10. Deviance in a GLM measures the discrepancy between observed data and predicted values, serving as a measure of model fit and used for model comparison and hypothesis testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression:\n",
    "\n",
    "11. What is regression analysis and what is its purpose?\n",
    "12. What is the difference between simple linear regression and multiple linear regression?\n",
    "13. How do you interpret the R-squared value in regression?\n",
    "14. What is the difference between correlation and regression?\n",
    "15. What is the difference between the coefficients and the intercept in regression?\n",
    "16. How do you handle outliers in regression analysis?\n",
    "17. What is the difference between ridge regression and ordinary least squares regression?\n",
    "18. What is heteroscedasticity in regression and how does it affect the model?\n",
    "19. How do you handle multicollinearity in regression analysis?\n",
    "20. What is polynomial regression and when is it used?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression:\n",
    "11. Regression analysis is a statistical technique used to model and analyze the relationship between a dependent variable and one or more independent variables, aiming to understand how changes in the independent variables affect the dependent variable.\n",
    "12. Simple linear regression involves analyzing the relationship between a single dependent variable and a single independent variable, while multiple linear regression involves analyzing the relationship between a single dependent variable and multiple independent variables.\n",
    "13. The R-squared value in regression represents the proportion of variance in the dependent variable that can be explained by the independent variables. It indicates the goodness of fit of the regression model.\n",
    "14. Correlation measures the strength and direction of the linear relationship between two variables, while regression analyzes the relationship between a dependent variable and independent variables, allowing for prediction and inference.\n",
    "15. Coefficients in regression represent the change in the dependent variable associated with a unit change in the corresponding independent variable, while the intercept represents the value of the dependent variable when all independent variables are zero.\n",
    "16. Outliers in regression can be handled by identifying and removing them, transforming the data, or using robust regression techniques.\n",
    "17. Ridge regression is a regularization technique that adds a penalty term to the ordinary least squares regression to handle multicollinearity and prevent overfitting.\n",
    "18. Heteroscedasticity refers to the unequal variance of residuals across different levels of the independent variables, and it can affect the accuracy of the regression model. It can be addressed through data transformation or using weighted least squares regression.\n",
    "19. Multicollinearity in regression occurs when independent variables are highly correlated, which can lead to unstable and unreliable coefficient estimates. It can be addressed by removing correlated variables or using regularization techniques.\n",
    "20. Polynomial regression is a regression technique that models the relationship between the dependent variable and independent variables as polynomial functions, allowing for curved relationships between variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function:\n",
    "\n",
    "21. What is a loss function and what is its purpose in machine learning?\n",
    "22. What is the difference between a convex and non-convex loss function?\n",
    "23. What is mean squared error (MSE) and how is it calculated?\n",
    "24. What is mean absolute error (MAE) and how is it calculated?\n",
    "25. What is log loss (cross-entropy loss) and how is it calculated?\n",
    "26. How do you choose the appropriate loss function for a given problem?\n",
    "27. Explain the concept of regularization in the context of loss functions.\n",
    "28. What is Huber loss and how does it handle outliers?\n",
    "29. What is quantile loss and when is it used?\n",
    "30. What is the difference between squared loss and absolute loss?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function:\n",
    "21. A loss function measures the discrepancy between predicted and actual values in machine learning models. It quantifies the model's performance and guides the learning process.\n",
    "22. A convex loss function has a single global minimum, making optimization easier, while a non-convex loss function has multiple local minima, making optimization more challenging.\n",
    "23. Mean squared error (MSE) is a loss function that measures the average squared difference between predicted and actual values.\n",
    "24. Mean absolute error (MAE) is a loss function that measures the average absolute difference between predicted and actual values.\n",
    "25. Log loss, also known as cross-entropy loss, is a loss function commonly used in classification tasks that quantifies the difference between predicted probabilities and actual binary outcomes.\n",
    "26. The choice of an appropriate loss function depends on the specific problem and the desired properties of the model. MSE is often used for regression, while log loss is used for binary classification.\n",
    "27. Regularization in loss functions introduces additional terms or penalties to prevent overfitting and improve generalization by shrinking or constraining the model's coefficients.\n",
    "28. Huber loss is a loss function that combines the benefits of squared loss and absolute loss, providing robustness to outliers.\n",
    "29. Quantile loss is a loss function used for quantile regression that measures the difference between predicted and actual quantiles.\n",
    "30. Squared loss emphasizes large errors more than absolute loss, making it more sensitive to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer (GD):\n",
    "\n",
    "31. What is an optimizer and what is its purpose in machine learning?\n",
    "32. What is Gradient Descent (GD) and how does it work?\n",
    "33. What are the different variations of Gradient Descent?\n",
    "34. What is the learning rate in GD and how do you choose an appropriate value?\n",
    "35. How does GD handle local optima in optimization problems?\n",
    "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
    "37. Explain the concept of batch size in GD and its impact on training.\n",
    "38. What is the role of momentum in optimization algorithms?\n",
    "39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
    "40. How does the learning rate affect the convergence of GD?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer (GD):\n",
    "31. An optimizer is an algorithm or method used to adjust the parameters of a machine learning model to minimize the loss function and improve the model's performance.\n",
    "32. Gradient Descent (GD) is an iterative optimization algorithm that updates the model's parameters in the direction of the steepest descent of the loss function.\n",
    "33. Different variations of GD include Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent, which differ in the amount of data used to update the parameters at each iteration.\n",
    "34. The learning rate in GD controls the step size or the rate at which the parameters are updated. An appropriate learning rate should be chosen to ensure convergence and prevent overshooting or slow convergence.\n",
    "35. GD can handle local optima through iterative updates and by exploring different directions based on the gradients of the loss function.\n",
    "36. Stochastic Gradient Descent (SGD) updates the parameters using a single randomly selected sample at each iteration, making it computationally efficient but introducing more noise.\n",
    "37. The batch size in GD refers to the number of samples used to compute the gradient and update the parameters at each iteration. It impacts the trade-off between computational efficiency and the stability of parameter updates.\n",
    "38. Momentum in optimization algorithms helps accelerate convergence by adding a fraction of the previous update to the current update, allowing the optimizer to navigate flatter regions of the loss function more effectively.\n",
    "39. Batch GD uses the entire dataset at each iteration, mini-batch GD uses a subset of the data, and SGD uses a single randomly selected sample. They differ in computational efficiency and convergence properties.\n",
    "40. The learning rate affects the convergence of GD by controlling the step size of parameter updates. A learning rate that is too high can lead to oscillation or divergence, while a learning rate that is too low can result in slow convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization:\n",
    "\n",
    "41. What is regularization and why is it used in machine learning?\n",
    "42. What is the difference between L1 and L2 regularization?\n",
    "43. Explain the concept of ridge regression and its role in regularization.\n",
    "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
    "45. How does regularization help prevent overfitting in machine learning models?\n",
    "46. What is early stopping and how does it relate to regularization?\n",
    "47. Explain the concept of dropout regularization in neural networks.\n",
    "48. How do you choose the regularization parameter in a model?\n",
    "49. What\n",
    "\n",
    " is the difference between feature selection and regularization?\n",
    "50. What is the trade-off between bias and variance in regularized models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization:\n",
    "41. Regularization is a technique used in machine learning to prevent overfitting by adding additional constraints or penalties to the model's loss function.\n",
    "42. L1 regularization adds an L1 penalty term to the loss function, which encourages sparsity and promotes feature selection by shrinking some coefficients to exactly zero.\n",
    "43. Ridge regression is a regression technique that incorporates L2 regularization by adding an L2 penalty term to the loss function. It helps address multicollinearity and reduces the impact of less important variables.\n",
    "44. Elastic Net regularization combines L1 and L2 penalties, allowing for a balance between sparsity and the shrinkage of coefficients.\n",
    "45. Regularization helps prevent overfitting by reducing the complexity of the model, controlling the coefficients' magnitudes, and improving generalization to\n",
    "\n",
    " unseen data.\n",
    "46. Early stopping is a regularization technique that stops the training process when the model's performance on a validation set starts to deteriorate, preventing overfitting.\n",
    "47. Dropout regularization is commonly used in neural networks, where random neurons are temporarily ignored during training to prevent over-reliance on specific neurons and improve generalization.\n",
    "48. The regularization parameter is typically determined through techniques like cross-validation, grid search, or model selection criteria, balancing the trade-off between model complexity and fit to the data.\n",
    "49. Feature selection focuses on selecting a subset of relevant features, while regularization shrinks the coefficients of less important features. Both techniques aim to improve model performance and interpretability.\n",
    "50. The bias-variance trade-off in regularized models refers to the trade-off between model bias (underfitting) and variance (overfitting). Regularization helps control this trade-off by reducing variance while introducing some bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:\n",
    "\n",
    "51. What is Support Vector Machines (SVM) and how does it work?\n",
    "52. How does the kernel trick work in SVM?\n",
    "53. What are support vectors in SVM and why are they important?\n",
    "54. Explain the concept of the margin in SVM and its impact on model performance.\n",
    "55. How do you handle unbalanced datasets in SVM?\n",
    "56. What is the difference between linear SVM and non-linear SVM?\n",
    "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
    "58. Explain the concept of slack variables in SVM.\n",
    "59. What is the difference between hard margin and soft margin in SVM?\n",
    "60. How do you interpret the coefficients in an SVM model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:\n",
    "51. Support Vector Machines (SVM) is a machine learning algorithm used for classification and regression tasks. It finds an optimal hyperplane or decision boundary that separates data points of different classes or predicts continuous values.\n",
    "52. The kernel trick in SVM allows for nonlinear decision boundaries by transforming the data into a higher-dimensional feature space, where linear separation is possible.\n",
    "53. Support vectors in SVM are the data points closest to the decision boundary. They play a crucial role in defining the decision boundary and determining the classification or regression outcome.\n",
    "54. The margin in SVM is the distance between the decision boundary and the support vectors. A larger margin indicates better generalization and higher robustness to noise.\n",
    "55. Unbalanced datasets in SVM can be handled by adjusting class weights, oversampling the minority class, undersampling the majority class, or using specialized techniques like SMOTE.\n",
    "56. Linear SVM uses a linear decision boundary, while non-linear SVM uses a kernel function to transform the data into a higher-dimensional feature space, allowing for nonlinear decision boundaries.\n",
    "57. The C-parameter in SVM controls the trade-off between achieving a wider margin and minimizing the misclassification of training examples. A smaller C results in a wider margin but potentially more misclassifications.\n",
    "58. Slack variables in SVM are introduced to handle non-separable data, allowing for some misclassifications within a certain margin.\n",
    "59. Hard margin SVM aims to find a decision boundary that perfectly separates the classes, while soft margin SVM allows for some misclassifications to handle overlapping or noisy data.\n",
    "60. The coefficients in an SVM model represent the importance of the support vectors and contribute to the definition of the decision boundary. Their signs indicate the classes they support.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees:\n",
    "\n",
    "61. What is a decision tree and how does it work?\n",
    "62. How do you make splits in a decision tree?\n",
    "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
    "64. Explain the concept of information gain in decision trees.\n",
    "65. How do you handle missing values in decision trees?\n",
    "66. What is pruning in decision trees and why is it important?\n",
    "67. What is the difference between a classification tree and a regression tree?\n",
    "68. How do you interpret the decision boundaries in a decision tree?\n",
    "69. What is the role of feature importance in decision trees?\n",
    "70. What are ensemble techniques and how are they related to decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees:\n",
    "61. A decision tree is a tree-like model that uses a sequence of decisions or splitting rules to classify or predict the value of a target variable based on features or attributes.\n",
    "62. Splits in a decision tree are determined based on feature values that result in the greatest reduction in impurity or the highest information gain.\n",
    "63. Impurity measures, such as Gini index or entropy, quantify the impurity or disorder of a node in a decision tree. Information gain is the difference in impurity before and after a split.\n",
    "64. Information gain in decision trees measures the reduction in impurity or disorder achieved by splitting a node based on a particular feature.\n",
    "65. Missing values in decision trees can be handled by different techniques, such as surrogate splits, assigning missing values to the most common class, or treating missing values as a separate category.\n",
    "66. Pruning in decision trees involves removing branches or nodes to simplify the tree, reduce overfitting, and improve generalization to unseen data.\n",
    "67. A classification tree predicts categorical or discrete outcomes, while a regression tree predicts continuous values.\n",
    "68. Decision boundaries in a decision tree are represented by the splits or branching points that determine how the input space is divided based on the feature values.\n",
    "69. Feature importance in decision trees represents the relative contribution of each feature in the decision-making process of the tree.\n",
    "70. Ensemble techniques, such as random forests or boosting, combine multiple decision trees to improve prediction accuracy and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Techniques:\n",
    "\n",
    "71. What are ensemble techniques in machine learning?\n",
    "72. What is bagging and how is it used in ensemble learning?\n",
    "73. Explain the concept of bootstrapping in bagging.\n",
    "74. What is boosting and how does it work?\n",
    "75. What is the difference between AdaBoost and Gradient Boosting?\n",
    "76. What is the purpose of random forests in ensemble learning?\n",
    "77. How do random forests handle feature importance?\n",
    "78. What is stacking in ensemble learning and how does it work?\n",
    "79. What are the advantages and disadvantages of ensemble techniques?\n",
    "80. How do you choose the optimal number of models in an ensemble?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Techniques:\n",
    "71. Ensemble techniques in machine learning combine multiple models to make predictions, leveraging the wisdom of crowds and improving prediction accuracy.\n",
    "72. Bagging is an ensemble technique that combines predictions from multiple models trained on different subsets of the training data through bootstrapping.\n",
    "73. Bootstrapping in bagging involves randomly sampling the training data with replacement to create multiple subsets, each used to train a separate model.\n",
    "74. Boosting is an ensemble technique that sequentially trains models on the training data, giving more weight to misclassified instances to focus on difficult cases.\n",
    "75. AdaBoost and Gradient Boosting are popular boosting algorithms that differ in the way they update model weights and combine multiple weak learners to create a strong learner.\n",
    "76. Random forests are an ensemble of decision trees where each tree is trained on a subset of the training data using random feature selection. They combine predictions from individual trees to make the final prediction.\n",
    "77. Random forests provide a measure of feature importance based on how much the prediction accuracy decreases when a particular feature is randomly permuted.\n",
    "78. Stacking is an ensemble technique that combines predictions from multiple models by training a meta-model that takes the outputs of the individual models as inputs.\n",
    "79. Ensemble techniques have advantages such as improved prediction accuracy, reduced overfitting, and handling complex relationships. Disadvantages include increased complexity and computational requirements.\n",
    "80. Determining the optimal number of models in an ensemble depends on the specific problem, the diversity of models, and the trade-off between model complexity and performance.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
